{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import h5py\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "from ttt.utils import listfiles\n",
    "from mlr.database.utils import read_metadata, write_metadata\n",
    "from matplotlib import cm\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import (IntSlider, RadioButtons, SelectMultiple, Layout, Checkbox, fixed, interact, FloatRangeSlider, Button)\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_history(path):\n",
    "    history_file = h5py.File(path,'r')\n",
    "    history_dict = {x:np.array(y) for x, y in history_file.items()} \n",
    "    history_file.close()\n",
    "    return history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_base = pathlib.Path(\"/media/files/segmentation_networks/\")\n",
    "root_dirs = [root_dir_base.joinpath(\"baseline_small_datasets_HQ_redux_1_6_2022\"),\n",
    "            root_dir_base.joinpath(\"small_NP_networks_redux_1_6_2022\"),\n",
    "            root_dir_base.joinpath(\"substrate_networks_redux_1_7_2022\"),\n",
    "            root_dir_base.joinpath(\"super_network_1_25_2022\"),\n",
    "            root_dir_base.joinpath(\"transfer_learning_all_with_callbacks_2_2_2022\")\n",
    "            ]\n",
    "\n",
    "root_dirs = [folder.joinpath(\"trained_models\") for folder in root_dirs]\n",
    "model_folders = [[x for x in listfiles(r) if x.is_dir()] for r in root_dirs]\n",
    "model_folders = reduce(lambda x,y: x+y, model_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load all the metadata from the networks\n",
    "metadata_list = []\n",
    "for folder in model_folders:\n",
    "    model_metadata = read_metadata(folder.joinpath(\"metadata.json\"))    \n",
    "    model_id = re.search(\"[0-9]+_[0-9]+\", str(folder).rsplit(\"/\")[-1])[0]\n",
    "    model_metadata[\"ID\"] = model_id\n",
    "\n",
    "    if \"transfer\" in str(folder):    \n",
    "        tl = True\n",
    "        history_path = \"sm_unet_transferLearnWeights_\" + model_metadata[\"backbone\"]+\"_history.h5\"\n",
    "        history_dict = read_training_history(folder.joinpath(history_path))\n",
    "        model_metadata.update(history_dict)\n",
    "        model_metadata[\"best_val_f1\"] = history_dict[\"val_f1-score\"][np.argmin(history_dict[\"val_loss\"])]\n",
    "        model_metadata[\"best_CdSe_f1\"] = history_dict[\"CdSe_f1-score\"][np.argmin(history_dict[\"CdSe_loss\"])]\n",
    "        model_metadata[\"best_Kath_f1\"] = history_dict[\"Kath_f1-score\"][np.argmin(history_dict[\"Kath_loss\"])]\n",
    "        model_metadata[\"CdSe_f1_at_val_peak\"] = history_dict[\"CdSe_f1-score\"][np.argmin(history_dict[\"val_loss\"])]\n",
    "        model_metadata[\"Kath_f1_at_val_peak\"] = history_dict[\"Kath_f1-score\"][np.argmin(history_dict[\"val_loss\"])]\n",
    "        model_metadata[\"series\"] = \"transfer\"\n",
    "        model_metadata[\"epoch\"] = np.array([i/len(history_dict[\"val_f1-score\"]) for i in np.arange(len(history_dict[\"val_f1-score\"]))])\n",
    "    else:\n",
    "        tl = False\n",
    "        model_metadata[\"series\"] = str(folder).rsplit(\"/\")[-3].split(\"_\")[0]\n",
    "    model_metadata[\"transfer_learned\"] = tl\n",
    "    model_metadata[\"folder\"] = folder\n",
    "    metadata_list.append(model_metadata)\n",
    "        \n",
    "df = pd.DataFrame(metadata_list)\n",
    "df = pd.concat([df, pd.DataFrame(df['schedule'].to_list(), columns = ['schedule_rate', 'schedule_timing'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_IDs = list(df[df[\"series\"]==\"baseline\"][\"ID\"])\n",
    "small_IDs = list(df[df[\"series\"]==\"small\"][\"ID\"])\n",
    "sub_IDs = list(df[df[\"series\"]==\"substrate\"][\"ID\"])\n",
    "super_IDs =  list(df[df[\"series\"]==\"super\"][\"ID\"])\n",
    "\n",
    "def return_orig_series(o_ID):\n",
    "    if o_ID in baseline_IDs:\n",
    "        return \"baseline\"\n",
    "    elif o_ID in small_IDs:\n",
    "        return \"small\"\n",
    "    elif o_ID in sub_IDs:\n",
    "        return \"substrate\"\n",
    "    elif o_ID in super_IDs:\n",
    "        return \"super\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def reduce_wrap(X):\n",
    "    try:\n",
    "        return reduce(lambda x,y: x+y, X)\n",
    "    except:\n",
    "        return np.NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"orig_series\"] =  df[\"orig_model_ID\"].apply(return_orig_series)\n",
    "df[\"substrate_thicknesses_str\"] = df[\"substrate_thicknesses\"].apply(reduce_wrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline can be matched on target dose\n",
    "\n",
    "SmallNP can be matched on N_defocus and N_structures\n",
    "\n",
    "Substrate can be matched on substrate_thicknesses\n",
    "\n",
    "Super networks can be matched on schedule_rate and alpha_0\n",
    "\n",
    "these should be propagated to the transfer learned networks' metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing transfer learning generalization and performance dynamics with training histories:\n",
    "\n",
    "For visualization:\n",
    "\n",
    "want a widget that has:\n",
    "- a slider for model ID selection [x]\n",
    "- a dropdown menu to select datasets [x]\n",
    "- a dropdown menu to select validation splits [x]\n",
    "- a radio button set to select normalization scheme [x]\n",
    "- a radio button to do either aggregate plotting (e.g., 5 faint lines per training base network) or averaged plotting (seaborn error bars?)\n",
    "- a checkbox to plot all models in filters (disables slider and plots all examples) [x]\n",
    "- checkbox to plot all models with same syntehtic training conditions [ ]\n",
    "\n",
    "\n",
    "Normalization can be either: \\\n",
    "A) fraction of best score achieved during transfer learning stage \\\n",
    "B) fraction of score before transfer learning \\\n",
    "C) fraction of max(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['alpha_0', 'backbone', 'batch_size', 'exp_f1-score',\n",
      "       'exp_f1-score_CdSe', 'exp_f1-score_katherine', 'exp_iou',\n",
      "       'exp_iou_CdSe', 'exp_iou_katherine', 'exp_loss', 'exp_loss_CdSe',\n",
      "       'exp_loss_katherine', 'schedule', 'seed', 'target_dose', 'ID', 'series',\n",
      "       'transfer_learned', 'folder', 'N_defocus', 'N_structures',\n",
      "       'substrate_thicknesses', 'expt_generator_seed', 'expt_val_split',\n",
      "       'orig_model_ID', 'CdSe_f1-score', 'CdSe_iou_score', 'CdSe_loss',\n",
      "       'Kath_f1-score', 'Kath_iou_score', 'Kath_loss', 'f1-score', 'iou_score',\n",
      "       'loss', 'lr', 'val_f1-score', 'val_iou_score', 'val_loss',\n",
      "       'best_val_f1', 'best_CdSe_f1', 'best_Kath_f1', 'CdSe_f1_at_val_peak',\n",
      "       'Kath_f1_at_val_peak', 'epoch', 'schedule_rate', 'schedule_timing',\n",
      "       'orig_series', 'substrate_thicknesses_str'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = df[df[\"transfer_learned\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global slider_size\n",
    "slider_size = len(plot_df)\n",
    "\n",
    "def filter_dataframe(data_frame, series, splits, plot_all_starters, plot_syn_cond, N):\n",
    "    filters = []\n",
    "    filters.append(reduce(lambda x,y: x | y, [data_frame[\"expt_val_split\"]==x for x in splits]))\n",
    "\n",
    "    if not \"All\" in series:\n",
    "        filters.append(reduce(lambda x,y: x | y, [data_frame[\"orig_series\"]==x.lower() for x in series]))\n",
    "\n",
    "    if plot_all_starters:\n",
    "        # filters.append(data_frame[\"orig_model_ID\"]==data_frame.iloc[N][\"orig_model_ID\"])\n",
    "        \n",
    "        tmp_data_frame = data_frame[reduce(lambda x,y: x & y, filters)]\n",
    "        return tmp_data_frame[tmp_data_frame[\"orig_model_ID\"]==tmp_data_frame.iloc[N][\"orig_model_ID\"]]\n",
    "    else:\n",
    "        return data_frame[reduce(lambda x,y: x & y, filters)]\n",
    "\n",
    "def match_training(data_frame, N):\n",
    "\n",
    "    o_series = data_frame.iloc[N][\"orig_series\"]\n",
    "\n",
    "    if o_series == \"baseline\":\n",
    "        keys = [\"target_dose\"]\n",
    "    elif o_series == \"small\":\n",
    "        keys = [\"N_defocus\", \"N_structures\"]\n",
    "    elif o_series == \"substrate\":\n",
    "        keys = [\"substrate_thicknesses_str\"]\n",
    "    elif o_series == \"super\":\n",
    "        keys = [\"schedule_rate\", \"alpha_0\"]\n",
    "\n",
    "    filters = [data_frame[k]==data_frame.iloc[N][k] for k in keys]\n",
    "    return data_frame[reduce(lambda x,y: x & y, filters)]\n",
    "\n",
    "def plot_row(data_frame, N, **kwargs):\n",
    "    r_df = filter_dataframe(data_frame, \n",
    "                            series=kwargs[\"dataset\"],\n",
    "                            splits=kwargs[\"val_split\"],\n",
    "                            plot_all_starters=kwargs[\"plot_all_starters\"],\n",
    "                            plot_syn_cond=kwargs[\"plot_syn_cond\"],\n",
    "                            N=N)\n",
    "        \n",
    "    global slider_size\n",
    "    slider_size = len(r_df)\n",
    "\n",
    "    if kwargs[\"plot_syn_cond\"]:\n",
    "        r_df = match_training(r_df, N)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    ax = fig.gca()\n",
    "    ax.set_ylim(kwargs[\"y_range\"])\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.grid()\n",
    "\n",
    "\n",
    "\n",
    "    # if plot all starters or aggregate plot, reduce line widthness and plot whole dataframe\n",
    "    # else, plot only the target row\n",
    "    if kwargs[\"plot_all_starters\"] or kwargs[\"aggregate_plot\"]:\n",
    "        rows = [x for __, x in r_df.iterrows()]\n",
    "        linewidth = 0.8\n",
    "    elif kwargs[\"plot_syn_cond\"]:\n",
    "        rows = [x for __, x in r_df.iterrows()]\n",
    "        linewidth = 1.2\n",
    "    else:\n",
    "        rows = [r_df.iloc[N]]\n",
    "        linewidth = 2.0\n",
    "\n",
    "    plot_keys = (\"val_f1-score\", \"CdSe_f1-score\", \"Kath_f1-score\")\n",
    "    key_map = {x:y for x, y in zip(plot_keys, (\"exp_f1-score\", \"exp_f1-score_CdSe\", \"exp_f1-score_katherine\"))}\n",
    "    plot_filter = {plot_keys[0]: \"Kate Au\" in kwargs[\"exp_dataset\"],\n",
    "                    plot_keys[1]: \"Kate CdSe\" in kwargs[\"exp_dataset\"],\n",
    "                    plot_keys[2]: \"Katherine Au\" in kwargs[\"exp_dataset\"],\n",
    "                    }\n",
    "\n",
    "    for row in rows:\n",
    "        for i, y in enumerate(plot_keys):\n",
    "            if plot_filter[y]:\n",
    "                if \"Max\" in kwargs[\"normalization\"]:\n",
    "                    div = np.max([row[key_map[y]], row[\"best_\"+y.rsplit(\"-\")[0]]])\n",
    "                elif \"Peak\" in kwargs[\"normalization\"]:\n",
    "                    div = row[\"best_\"+y.rsplit(\"-\")[0]]\n",
    "                elif \"Synthetic\" in kwargs[\"normalization\"]:\n",
    "                    div = row[key_map[y]]\n",
    "                else:\n",
    "                    div = 1\n",
    "\n",
    "                if kwargs[\"shift\"]:\n",
    "                    shift = row[key_map[y]]\n",
    "                else:\n",
    "                    shift = 0.0\n",
    "\n",
    "                ax.plot(row[\"epoch\"], (row[y]-shift)/div, color=sns.color_palette(\"pastel\")[i], linewidth=linewidth)\n",
    "\n",
    "    custom_lines = [Line2D([0], [0], color=sns.color_palette(\"pastel\")[0], lw=4),\n",
    "                Line2D([0], [0], color=sns.color_palette(\"pastel\")[1], lw=4),\n",
    "                Line2D([0], [0], color=sns.color_palette(\"pastel\")[2], lw=4)]\n",
    "\n",
    "    ax.legend(custom_lines, [\"Kate Au\", \"Kate CdSe\", \"Katherine Au\"],\n",
    "                loc=\"upper right\",\n",
    "                bbox_to_anchor=(1.18,1.02))\n",
    "    ax.axhline(0.0, color=(1,0,0), linestyle=\"--\")\n",
    "    ax.axhline(1.0, color=(1,0,0), linestyle=\"--\")\n",
    "    ax.set_xlabel(\"Normalized epoch\")\n",
    "    ax.set_ylabel(\"Shift in Dice Score\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_lines_widget(data_frame):\n",
    "    w_data_filter = SelectMultiple(\n",
    "        options=['All', 'Baseline', 'Small', 'Substrate', 'Super'],\n",
    "        value=('All',),\n",
    "        description='Syn. Dataset',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    w_exp_filter = SelectMultiple(\n",
    "        options=['Kate Au', 'Kate CdSe', 'Katherine Au'],\n",
    "        value=('Kate Au', 'Kate CdSe', 'Katherine Au'),\n",
    "        description='Exp. Dataset',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    w_normalization =RadioButtons(\n",
    "            options=['Synthetic Score', 'Peak Transfer Score', 'Max(Synthetic, Transfer)', 'None'],\n",
    "            description='Normalization',\n",
    "            disabled=False\n",
    "        )\n",
    "\n",
    "    w_split_filter = SelectMultiple(\n",
    "        options=[0.2, 0.4, 0.5, 0.6, 0.8],\n",
    "        value=(0.2,),\n",
    "        description='Val. Data Split',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    w_line_select = IntSlider(value=0, min=0, max=slider_size-1, description=\"Model #\")\n",
    "\n",
    "    w_starting_network = Checkbox(value=False, description=\"Plot same starting network\", disabled=True)\n",
    "    w_training_conditions = Checkbox(value=False, description=\"Plot same synthetic training\")\n",
    "    w_aggregate = Checkbox(value=False, description=\"Aggregate plot\")\n",
    "    w_y_range = FloatRangeSlider(value=[0, 2.0],\n",
    "                                min=-1.0,\n",
    "                                max=2.5,\n",
    "                                step=0.1,\n",
    "                                description='y-axis range:',\n",
    "                                disabled=False,\n",
    "                                continuous_update=False,\n",
    "                                orientation='horizontal',\n",
    "                                readout=True,\n",
    "                                readout_format='.1f',\n",
    "                            )\n",
    "\n",
    "    w_shift = Checkbox(value=False, description=\"Shift by synthetic score\")\n",
    "\n",
    "    return interact(plot_row, data_frame=fixed(data_frame),\n",
    "                             N=w_line_select,\n",
    "                             dataset=w_data_filter,\n",
    "                             val_split=w_split_filter,\n",
    "                             exp_dataset=w_exp_filter,\n",
    "                             normalization=w_normalization,\n",
    "                             shift=w_shift,\n",
    "                             plot_all_starters=w_starting_network,\n",
    "                             plot_syn_cond=w_training_conditions,\n",
    "                             aggregate_plot=w_aggregate,\n",
    "                             y_range=w_y_range,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86f2338a4494008a49d00347df56544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Model #', max=1274), SelectMultiple(description='Syn. Da…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W = plot_lines_widget(plot_df)\n",
    "\n",
    "idx = -1\n",
    "for i, child in enumerate(W.widget.children):\n",
    "    if isinstance(child, IntSlider):\n",
    "        idx = i\n",
    "        break\n",
    "\n",
    "slider = W.widget.children[idx]\n",
    "\n",
    "def on_value_change(change):\n",
    "    global slider_size\n",
    "    slider.max = slider_size-1\n",
    "\n",
    "for child in W.widget.children:\n",
    "    child.observe(on_value_change, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(center-neighbor)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5a95364771b75b43ed7d376f561a8b7af4a99cf935179260fe8e22ddd449456"
  },
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
